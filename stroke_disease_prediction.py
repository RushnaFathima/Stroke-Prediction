# -*- coding: utf-8 -*-
"""STROKE DISEASE PREDICTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FavVd7HyLjQd6q5fxwOMDKzOLm95l_4z

1.IMPORTING THE DEPENDENCIES
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns

"""2.DATA LOADING AND UNDERSTANDING"""

stroke_data=pd.read_csv("/content/healthcare-dataset-stroke-data (1) (1).csv")

stroke_data.head(5)

stroke_data.shape

stroke_data.isnull().sum()

stroke_data.info()

# convert age column datatype to integer
stroke_data["age"] = stroke_data["age"].astype(int)

mean_bmi = stroke_data['bmi'].mean()
stroke_data['bmi'].fillna(mean_bmi, inplace=True)
#stroke_data.dropna(inplace=True)

stroke_data.describe().T

for i in stroke_data.columns:
  print(i)

for col in stroke_data.columns:
  numerical_features = ["id", "age", "bmi","avg_glucose_level"]
  if col not in numerical_features:
    print(col,":", stroke_data[col].unique(),"\n")

stroke_data.drop(["id"],axis=1,inplace=True)

stroke_data["stroke"].value_counts()

# Histogram for "age"

sns.histplot(stroke_data["age"], kde=True)
plt.title("Distribution of Age")

# calculate mean and median
age_mean = stroke_data["age"].mean()
age_median = stroke_data["age"].median()

print("Mean:", age_mean)
print("Median:", age_median)


# add vertical lines for mean and median
plt.axvline(age_mean, color="red", linestyle="--", label="Mean")
plt.axvline(age_median, color="green", linestyle="-", label="Median")

plt.legend()

plt.show()

# Histogram for "bmi"

sns.histplot(stroke_data["bmi"], kde=True)
plt.title("Distribution of bmi")

# calculate mean and median
bmi_mean = stroke_data["bmi"].mean()
bmi_median = stroke_data["bmi"].median()

print("Mean:", bmi_mean)
print("Median:", bmi_median)


# add vertical lines for mean and median
plt.axvline(bmi_mean, color="red", linestyle="--", label="Mean")
plt.axvline(bmi_median, color="green", linestyle="-", label="Median")

plt.legend()

plt.show()

# Histogram for "avg_glucose_level"

sns.histplot(stroke_data["avg_glucose_level"], kde=True)
plt.title("Distribution of avg_glucose_level")

# calculate mean and median
avg_glucose_level_mean = stroke_data["avg_glucose_level"].mean()
avg_glucose_level_median = stroke_data["avg_glucose_level"].median()

print("Mean:", avg_glucose_level_mean)
print("Median:", avg_glucose_level_median)


# add vertical lines for mean and median
plt.axvline(avg_glucose_level_mean, color="red", linestyle="--", label="Mean")
plt.axvline(avg_glucose_level_median, color="green", linestyle="-", label="Median")

plt.legend()

plt.show()

# box plot
sns.boxplot(x=stroke_data["age"])
plt.title("Box Plot for Age")
plt.xlabel("Age")
plt.show()

# box plot
sns.boxplot(x=stroke_data["bmi"])
plt.title("Box Plot for bmi")
plt.xlabel("bmi")
plt.show()

# box plot
sns.boxplot(x=stroke_data["avg_glucose_level"])
plt.title("Box Plot for avg_glucose_level")
plt.xlabel("avg_glucose_level")
plt.show()

# count the outliers using IQR method
numerical_features=[ "bmi","avg_glucose_level","age"]
for col in numerical_features:
  Q1 = stroke_data[col].quantile(0.25)
  Q3 = stroke_data[col].quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  f"{col}_outliers"== stroke_data[(stroke_data[col] < lower_bound) | (stroke_data[col] > upper_bound)]
  print(f"{col}_outliers_count=",len(f"{col}_outliers"))

import pandas as pd
import matplotlib.pyplot as plt

# Define numeric columns (we'll skip these)
numerical_features = ["id", "age", "bmi", "avg_glucose_level"]

# Loop through all categorical columns
for col in stroke_data.columns:
    if col not in numerical_features and col != "stroke":

        # Count stroke and non-stroke per category
        count_df = stroke_data.groupby(col)["stroke"].value_counts().unstack(fill_value=0)

        # Plot stacked bars (blue shades)
        ax = count_df.plot(
            kind="bar",
            stacked=True,
            color=["#9ecae1", "#08519c"],  # light blue & dark blue
            figsize=(7, 4)
        )

        # Titles and labels
        plt.title(f"Count Plot for {col}", fontsize=12)
        plt.xlabel(col, fontsize=10)
        plt.ylabel("Count", fontsize=10)
        plt.legend(title="Stroke", labels=["No Stroke", "Stroke"])

        # Add percentage labels inside bars
        for i, (index, row) in enumerate(count_df.iterrows()):
            total = row.sum()
            bottom = 0
            for j, val in enumerate(row):
                percentage = val / total * 100 if total > 0 else 0
                ax.text(
                    i,
                    bottom + val / 2,
                    f"{percentage:.1f}%",
                    ha="center",
                    va="center",
                    color="white",
                    fontsize=9,
                    fontweight="bold"
                )
                bottom += val

        plt.tight_layout()
        plt.show(
        )

#data splitting to dependent and independent variables
X=stroke_data.drop(["stroke"],axis=1)#independent
y=stroke_data["stroke"]#dependent

X.head(5)

y.head(5)

X.dtypes

#one-hot encoding
X_encoded = pd.get_dummies(X,columns=['gender','ever_married', 'work_type', 'Residence_type', 'smoking_status'],dtype=int)

X_encoded.head()

X_encoded.columns

import joblib
expected_cols = X_encoded.columns.tolist()
joblib.dump(expected_cols, "model_columns.pkl")

sum(y)/len(y)

"""So we see that 4.857% of people in the dataset have stroke."""

X_train, X_test, y_train, y_test = train_test_split(X_encoded,y,test_size=0.3,random_state=42,stratify=y)

sum(y_train)/len(y_train)

sum(y_test)/len(y_test)

"""# **MODEL TRAINING AND EVALUATION**

**XGBoost**

RandomizedSearchCV
"""

import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV
import scipy.stats as stats

# Define the hyperparameter distributions
param_dist = {
    'max_depth': stats.randint(3, 10),
    'learning_rate': stats.uniform(0.01, 0.09),
    'subsample': stats.uniform(0.5, 0.5),
    'colsample_bytree': stats.uniform(0.5, 0.5),
    'gamma': stats.uniform(0, 5),
    'min_child_weight': stats.randint(1, 10),
    'n_estimators': stats.randint(50, 200),
    'reg_lambda': stats.uniform(0, 10),        # L2 regularization
    'scale_pos_weight': stats.uniform(1, 10)   # imbalance handling
}
# Create the XGBoost model object
xgb_model  = xgb.XGBClassifier(
    eval_metric="aucpr",   # use PR-AUC for imbalance
    random_state=42
)


# Create the RandomizedSearchCV object
random_search = RandomizedSearchCV(
    xgb_model,
    param_distributions=param_dist,
    n_iter=50,
    cv=5,
    scoring='recall',
    n_jobs=10,
    random_state=42,
)

# Fit the RandomizedSearchCV object to the training data
random_search.fit(X_train, y_train)

# Print the best set of hyperparameters and the corresponding score
print("Best set of hyperparameters: ", random_search.best_params_)
print("Best score: ", random_search.best_score_)

random_search.fit(X_train, y_train)

model_rs = random_search.best_estimator_

# Predict on the test set (replace with your X_test, y_test)
y_pred_xgb = model_rs .predict(X_test)

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred_xgb, labels=model_rs.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_rs.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.grid(False)
plt.title("Confusion Matrix")
plt.show()

# Import necessary libraries
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Accuracy
accuracy = accuracy_score(y_test, y_pred_xgb)
print(f"Accuracy: {accuracy:.4f}")

# Precision
precision = precision_score(y_test, y_pred_xgb)
print(f"Precision: {precision:.4f}")

# Recall
recall = recall_score(y_test, y_pred_xgb)
print(f"Recall: {recall:.4f}")

# F1 Score
f1 = f1_score(y_test, y_pred_xgb)
print(f"F1 Score: {f1:.4f}")

# AUC Score
auc = roc_auc_score(y_test, y_pred_xgb)
print(f"AUC Score: {auc:.4f}")

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

# Use best model from RandomizedSearchCV
model_rs = random_search.best_estimator_

# Get predicted probabilities (for the positive class)
y_proba = model_rs.predict_proba(X_test)[:, 1]

# Compute ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)

# Compute AUC
auc_score = roc_auc_score(y_test, y_proba)

# Plot
plt.figure(figsize=(6,6))
plt.plot(fpr, tpr, color="blue", label=f"ROC curve (AUC = {auc_score:.3f})")
plt.plot([0,1], [0,1], color="gray", linestyle="--")  # diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend(loc="lower right")
plt.show()

from imblearn.over_sampling import SMOTE

# Instantiate SMOTE
sm = SMOTE(random_state=42)

# Fit SMOTE to training data
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

# Print class distribution of original and resampled data
print('Class distribution before resampling:', y_train.value_counts())
print('Class distribution after resampling:', y_train_res.value_counts())

plt.subplot(1, 2, 2)
ax = sns.countplot(x=y_train, color="black")
plt.xlabel('Stroke (0 = No, 1 = Yes)', fontsize=11)
plt.ylabel('Count', fontsize=11)

# Add black count labels on each bar
for p in ax.patches:
    height = p.get_height()
    ax.text(
        p.get_x() + p.get_width() / 2.,   # X position (center of bar)
        height + 50,                      # Y position (slightly above bar)
        f'{int(height)}',                 # Text label
        ha='center', va='bottom',
        fontsize=11
    )

plt.tight_layout()
plt.show()

plt.subplot(1, 2, 2)
ax = sns.countplot(x=y_train_res, color="black")
plt.xlabel('Stroke (0 = No, 1 = Yes)', fontsize=11)
plt.ylabel('Count', fontsize=11)

# Add black count labels on each bar
for p in ax.patches:
    height = p.get_height()
    ax.text(
        p.get_x() + p.get_width() / 2.,   # X position (center of bar)
        height + 50,                      # Y position (slightly above bar)
        f'{int(height)}',                 # Text label
        ha='center', va='bottom',
        fontsize=11
    )

plt.tight_layout()
plt.show()

random_search.fit(X_train_res, y_train_res)

# Print the best set of hyperparameters and the corresponding score
print("Best set of hyperparameters: ", random_search.best_params_)
print("Best score: ", random_search.best_score_)

random_search.fit(X_train_res, y_train_res)

# Use the best estimator from RandomizedSearchCV
model_rs_smote = random_search.best_estimator_

# Predict on the test set (replace with your X_test, y_test)
y_pred_xgb_s = model_rs_smote.predict(X_test)

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred_xgb_s, labels=model_rs_smote.classes_)

# Plot
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_rs_smote.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("XGBoost")
plt.grid(False)
plt.show()

y_proba = model_rs_smote.predict_proba(X_test)[:, 1]

# Compute ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)

# Compute AUC
auc_score = roc_auc_score(y_test, y_proba)

# Plot
plt.figure(figsize=(6,6))
plt.plot(fpr, tpr, color="blue", label=f"ROC curve (AUC = {auc_score:.3f})")
plt.plot([0,1], [0,1], color="gray", linestyle="--")  # diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("XGBoost")
plt.legend(loc="lower right")
plt.show()



# Accuracy
accuracy = accuracy_score(y_test, y_pred_xgb_s)
print(f"Accuracy: {accuracy:.4f}")

# Precision
precision = precision_score(y_test, y_pred_xgb_s)
print(f"Precision: {precision:.4f}")

# Recall
recall = recall_score(y_test, y_pred_xgb_s)
print(f"Recall: {recall:.4f}")

# F1 Score
f1 = f1_score(y_test, y_pred_xgb_s)
print(f"F1 Score: {f1:.4f}")

# AUC Score
auc = roc_auc_score(y_test, y_pred_xgb_s)
print(f"AUC Score: {auc:.4f}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
print("Accuracy score:", accuracy_score(y_test, y_pred_xgb_s))
print("Classification Report:\n", classification_report(y_test, y_pred_xgb_s))

from imblearn.combine import SMOTETomek
import collections
smt = SMOTETomek(random_state=42)
X_res_st, y_res_st = smt.fit_resample(X_train, y_train)
print("Resampled dataset shape:", collections.Counter(y_res_st))

random_search.fit(X_res_st, y_res_st)
# Print the best set of hyperparameters and the corresponding score
print("Best set of hyperparameters: ", random_search.best_params_)
print("Best score: ", random_search.best_score_)

model_rs_s = random_search.best_estimator_
y_pred_s = model_rs_s.predict(X_test)

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred_s, labels=model_rs_s.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_rs_s.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("XGBoost")
plt.grid(False)
plt.show()
print("Accuracy score:\n", accuracy_score(y_test, y_pred_s))
print("Classification Report:\n", classification_report(y_test, y_pred_s))

importances = model_rs_s.feature_importances_
features = X_encoded.columns
feat_df = pd.DataFrame({"Feature": features, "Importance": importances})
feat_df = feat_df.sort_values(by="Importance", ascending=False)
# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feat_df)
plt.title("Feature Importance - Random Forest")
plt.show()

import joblib

joblib.dump(model_rs_st, "xgb_stroke_model.pkl")

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
# dictionary of classifiers
models = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42)
}

from sklearn.model_selection import train_test_split, cross_val_score
# dictionary to store the cross validation results
cv_scores = {}

# perform 5-fold cross validation for each model
for model_name, model in models.items():
  print(f"Training {model_name} with default parameters...")
  scores = cross_val_score(model, X_train_res, y_train_res, cv=5, scoring="accuracy")
  cv_scores[model_name] = scores
  print(f"{model_name} Cross-Validation Accuracy: {np.mean(scores):.2f}")
  print("-"*50)

cv_scores

# Initializing models
decision_tree = DecisionTreeClassifier(random_state=42)
random_forest = RandomForestClassifier(random_state=42)

"""**DECISION TREE**"""

from scipy.stats import randint
from sklearn.metrics import accuracy_score
# ----------------- Decision Tree -----------------
param_dist_dt = {
    "criterion": ["gini", "entropy"],
    "max_depth": [None] + list(range(5, 71)),   # None + depths 5 → 70
    "min_samples_split": randint(2, 21),        # integers from 2 → 20
    "min_samples_leaf": randint(1, 9)           # integers from 1 → 8
}
random_dt = RandomizedSearchCV(
    estimator=DecisionTreeClassifier(random_state=42),
    param_distributions=param_dist_dt,
    n_iter=20,         # number of random combos to try
    cv=5,
    scoring="recall",
    random_state=42,
    n_jobs=-1,
    verbose=2
)

random_dt.fit(X_train_res, y_train_res)

#balanced data
print("Best Decision Tree Params:", random_dt.best_params_)
print("Best CV Accuracy:", random_dt.best_score_)

y_pred_dt = random_dt.predict(X_test)
cm = confusion_matrix(y_test, y_pred_dt)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Decision Tree")
plt.grid(False)
plt.show()
print("Accuracy score:\n", accuracy_score(y_test, y_pred_dt))
print("Classification Report:\n", classification_report(y_test, y_pred_dt))

# Accuracy
accuracy = accuracy_score(y_test, y_pred_dt)
print(f"Accuracy: {accuracy:.4f}")

# Precision
precision = precision_score(y_test, y_pred_dt)
print(f"Precision: {precision:.4f}")

# Recall
recall = recall_score(y_test, y_pred_dt)
print(f"Recall: {recall:.4f}")

# F1 Score
f1 = f1_score(y_test, y_pred_dt)
print(f"F1 Score: {f1:.4f}")

# AUC Score
auc = roc_auc_score(y_test, y_pred_dt)
print(f"AUC Score: {auc:.4f}")

"""**RANDOM FOREST**"""

# ----------------- Random Forest -----------------

param_dist_rf = {
    "n_estimators": randint(50, 500),        # random integers between 50 and 500
    "max_depth": randint(5, 50),             # random integers between 5 and 50
    "min_samples_split": randint(2, 20),     # random integers between 2 and 20
    "min_samples_leaf": randint(1, 10),      # random integers between 1 and 10
    "bootstrap": [True, False]               # categorical, so still a list
}

random_rf = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_dist_rf,
    n_iter=30,         # number of random combos to try
    cv=5,
    scoring="recall",
    random_state=42,
    n_jobs=-1,
    verbose=2
)

random_rf.fit(X_train_res, y_train_res)

print("Best Random Forest Params:", random_rf.best_params_)
print("Best CV Accuracy:", random_rf.best_score_)

y_pred_rf = random_rf.predict(X_test)

cm = confusion_matrix(y_test, y_pred_rf)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Random Forest")
plt.grid(False)
plt.show()
print("Test Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf))

y_proba = random_dt.predict_proba(X_test)[:, 1]

# Compute ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)

# Compute AUC
auc_score = roc_auc_score(y_test, y_proba)

# Plot
plt.figure(figsize=(6,6))
plt.plot(fpr, tpr, color="blue", label=f"ROC curve (AUC = {auc_score:.3f})")
plt.plot([0,1], [0,1], color="gray", linestyle="--")  # diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Decision Tree")
plt.legend(loc="lower right")
plt.show()

y_proba = random_rf.predict_proba(X_test)[:, 1]

# Compute ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)

# Compute AUC
auc_score = roc_auc_score(y_test, y_proba)

# Plot
plt.figure(figsize=(6,6))
plt.plot(fpr, tpr, color="blue", label=f"ROC curve (AUC = {auc_score:.3f})")
plt.plot([0,1], [0,1], color="gray", linestyle="--")  # diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Random Forest")
plt.legend(loc="lower right")
plt.show()

# Accuracy
accuracy = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy: {accuracy:.4f}")

# Precision
precision = precision_score(y_test, y_pred_rf)
print(f"Precision: {precision:.4f}")

# Recall
recall = recall_score(y_test, y_pred_rf)
print(f"Recall: {recall:.4f}")

# F1 Score
f1 = f1_score(y_test, y_pred_rf)
print(f"F1 Score: {f1:.4f}")

# AUC Score
auc = roc_auc_score(y_test, y_pred_rf)
print(f"AUC Score: {auc:.4f}")